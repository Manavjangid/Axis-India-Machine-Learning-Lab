{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Binary_Logistic_Regression import BinaryLogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('aps_failure_training_set.csv', skiprows=20)\n",
    "testing_data = pd.read_csv('aps_failure_test_set.csv', skiprows=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scania_APS_Failure(BinaryLogisticRegression):\n",
    "    def __init__(self,training, testing, n_comp, non_na_thresh, LR=10**(-6), ET=10**(-5)):\n",
    "        \n",
    "        self.n_comp = n_comp\n",
    "        self.training = training.copy()\n",
    "        self.testing = testing.copy()\n",
    "        \n",
    "        self.training.replace(to_replace='na', value=np.nan, inplace=True)\n",
    "        self.testing.replace(to_replace='na', value=np.nan, inplace=True)\n",
    "        \n",
    "        self.training.dropna(axis=1, thresh=int(non_na_thresh*self.training.shape[0]), inplace=True)\n",
    "        self.testing = self.testing[self.training.columns]\n",
    "        \n",
    "        training_label = self.training['class']\n",
    "        training_label.replace(to_replace=['pos','neg'], value=[1,0], inplace=True)\n",
    "        testing_label = self.testing['class']\n",
    "        testing_label.replace(to_replace=['pos','neg'], value=[1,0], inplace=True)\n",
    "        \n",
    "        imputer = SimpleImputer()\n",
    "        imputed_training = imputer.fit_transform(self.training.iloc[:,1:])\n",
    "        imputed_testing = imputer.fit_transform(self.testing.iloc[:,1:])\n",
    "        \n",
    "        self.training = pd.DataFrame(imputed_training, columns=self.training.columns[1:])\n",
    "        self.testing = pd.DataFrame(imputed_testing, columns=self.testing.columns[1:])\n",
    "        \n",
    "        for column in self.training:\n",
    "            self.training[column] = pd.qcut(x=self.training[column], q=10, duplicates='drop').cat.codes\n",
    "            self.testing[column] = pd.qcut(x=self.testing[column], q=10, duplicates='drop').cat.codes\n",
    "        \n",
    "        self.training = self.__one_hot_encoding(self.training)\n",
    "        self.testing = self.__one_hot_encoding(self.testing)\n",
    "        self.testing['ad_000_8'] = np.array([0 for i in range(self.testing.shape[0])])\n",
    "        \n",
    "        self.training = (self.training - self.training.mean(axis=0))/self.training.std(axis=0)\n",
    "        self.testing = (self.testing - self.testing.mean(axis=0))/self.testing.std(axis=0)\n",
    "        \n",
    "        self.training.dropna(axis=1, thresh=int(non_na_thresh*self.training.shape[0]), inplace=True)\n",
    "        self.testing = self.testing[self.training.columns]\n",
    "        \n",
    "        self.__eigen_vector_calculation()\n",
    "        \n",
    "        self.training = np.matmul(np.array(self.training),self.Q)\n",
    "        self.testing = np.matmul(np.array(self.testing),self.Q)\n",
    "        \n",
    "        self.training, training_label = SMOTE(sampling_strategy='minority').fit_resample(X=self.training, y=training_label)\n",
    "        self.training = pd.DataFrame(self.training)\n",
    "        self.training['class'] = training_label\n",
    "        \n",
    "        self.testing = pd.DataFrame(self.testing)\n",
    "        self.testing['class'] = testing_label\n",
    "        self.testing['class'].dropna(axis=0, inplace=True)\n",
    "        \n",
    "        X_cv, X_test, Y_cv, Y_test = train_test_split(self.testing.iloc[:,:-1], self.testing['class'], test_size=1/3)\n",
    "        \n",
    "        self.cv = pd.DataFrame(X_cv)\n",
    "        self.cv['class'] = Y_cv\n",
    "        \n",
    "        self.testing = pd.DataFrame(X_test)\n",
    "        self.testing['class'] = Y_test\n",
    "        \n",
    "        super().__init__(1, LearningRate=LR, ErrorTolerance=ET)\n",
    "        \n",
    "        del X_cv, X_test, Y_cv, Y_test, column, imputed_testing, imputed_training, imputer, n_comp, non_na_thresh, testing_label, training_label\n",
    "        \n",
    "        \n",
    "    def __eigen_vector_calculation(self):\n",
    "        data = np.array(self.training)\n",
    "        sigma_hat = (1/self.training.shape[0]) * np.matmul(data.T,data)\n",
    "        self.Q = np.linalg.svd(sigma_hat)[0][:,:self.n_comp]\n",
    "        \n",
    "    def __one_hot_encoding(self, df):\n",
    "        data_array = list()\n",
    "        column_name = list()\n",
    "        for column in df.columns:\n",
    "            unique = df[column].unique()\n",
    "            size = len(df[column].unique())\n",
    "            df[column].replace(to_replace=unique, value=range(size), inplace=True)\n",
    "            data_array.append(np.eye(size, size)[df[column]])\n",
    "            for u in unique:\n",
    "                column_name.append(column + '_' + str(u))\n",
    "        return pd.DataFrame(np.concatenate(data_array, axis=1), columns=column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predicted, actual):\n",
    "    TP = np.count_nonzero((predicted == 1) & (actual == 1))\n",
    "    TN = np.count_nonzero((predicted == 0) & (actual == 0))\n",
    "    FP = np.count_nonzero((predicted == 1) & (actual == 0))\n",
    "    FN = np.count_nonzero((predicted == 0) & (actual == 1))\n",
    "    \n",
    "    if (TP + TN + FP + FN) == 0:\n",
    "        accuracy = 0\n",
    "    else:\n",
    "        accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    \n",
    "    if (TP + FP) == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = TP/(TP + FP)\n",
    "    \n",
    "    if (TP + FN) == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = TP/(TP + FN)\n",
    "        \n",
    "    if (precision + recall) == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = (2 * precision * recall)/(precision + recall)\n",
    "    \n",
    "    return (accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = dict()\n",
    "best = {'Accuracy':0, 'Precision':0, 'Recall':0, 'F1 Score':0, 'Best Obj':'obj', 'Parameter':()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Different Combinaitions of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for non_na_thresh in np.arange(0.50, 0.96, 0.05): \n",
    "    for n_comp in range(2,537,19):\n",
    "        for LR in [5,6,7,8,9,10]:\n",
    "            for ET in [5,6,7,8,9,10]:\n",
    "                obj = Scania_APS_Failure(training_data, testing_data, n_comp, non_na_thresh, LR=LR, ET=ET)\n",
    "                obj.fit(obj.training.iloc[:,:-1],obj.training['class'])\n",
    "                obj.predict(obj.cv.iloc[:,:-1])\n",
    "                Results[(non_na_thresh, n_comp, f'10**(-{LR})', f'10**(-{ET})')] = evaluate(obj.predicted_labels, obj.cv['class'])\n",
    "                if (Results[(non_na_thresh, n_comp, f'10**(-{LR})', f'10**(-{ET})')][0] + Results[(non_na_thresh, n_comp, f'10**(-{LR})', f'10**(-{ET})')][1] + Results[(non_na_thresh, n_comp, f'10**(-{LR})', f'10**(-{ET})')][2]) > (best['Accuracy'] + best['Precision'] + best['Recall']):\n",
    "                    best['Accuracy'] = Results[(non_na_thresh, n_comp, f'10**(-{LR})', f'10**(-{ET})')][0]\n",
    "                    best['Precision'] = Results[(non_na_thresh, n_comp, f'10**(-{LR})', f'10**(-{ET})')][1]\n",
    "                    best['Recall'] = Results[(non_na_thresh, n_comp, f'10**(-{LR})', f'10**(-{ET})')][2]\n",
    "                    best['F1 Score'] = Results[(non_na_thresh, n_comp, f'10**(-{LR})', f'10**(-{ET})')][3]\n",
    "                    best['Best Obj'] = obj\n",
    "                    best['Parameter'] = (non_na_thresh, n_comp, f'10**(-{LR})', f'10**(-{ET})')\n",
    "                else:\n",
    "                    del obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From over 10440 Hyperparameter combinaitons we find our best modelÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_obj = best['Best Obj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_obj.predict(best_obj.testing.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(best_obj.predicted_labels, best_obj.testing['class'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit16fa44404e4941f7996ce5d53eb37087"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
