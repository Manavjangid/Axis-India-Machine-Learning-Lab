{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from the following link:\n",
    "\n",
    "https://drive.google.com/open?id=0B4c_CWtMboiGRHJhWWZMbGpQazFxcHNfUkhnQ1pQU09IeXFR\n",
    "\n",
    "If you read the dataset by importing pandas library and using pd.read_csv():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the first column 'id' and the last column 'Unnamed:32' from this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels=[data.columns[0],data.columns[32]],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to determine the best fitting probability distribution on each column data (Sample Data of the Random Variable) of the dataset, out of the possible choices of probability distributions: Normal(Symmetric), Rayleigh(Skewed) or Binomial. \n",
    "\n",
    "So, let's see that how are we going to do that for a single column data and then we have to repeat this process for rest of the columns: \n",
    "\n",
    "Step 1: First, we will check whether the Random Variable is Discrete or Continous by checking that how many unique values are there in the single column of the Random Variable and does the Random Variable has string or floating point or integer values? Usually, Discrete Random Variables have either Integer or String values whereas continous random variables have floating point values and also because the values are not sparse (0s or 1s) in continous random variables and they are dense (floating point values) so values don't repeat much and hence the number of unique values are almost equal to the total number of values in the column. As an example, lets take one by one two columns: 'diagnosis' and 'radius_mean': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_unique_values = data['diagnosis'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_mean_unique_values = data['radius_mean'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'B'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosis_unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.99 , 20.57 , 19.69 , 11.42 , 20.29 , 12.45 , 18.25 , 13.71 ,\n",
       "       13.   , 12.46 , 16.02 , 15.78 , 19.17 , 15.85 , 13.73 , 14.54 ,\n",
       "       14.68 , 16.13 , 19.81 , 13.54 , 13.08 ,  9.504, 15.34 , 21.16 ,\n",
       "       16.65 , 17.14 , 14.58 , 18.61 , 15.3  , 17.57 , 18.63 , 11.84 ,\n",
       "       17.02 , 19.27 , 16.74 , 14.25 , 13.03 , 14.99 , 13.48 , 13.44 ,\n",
       "       10.95 , 19.07 , 13.28 , 13.17 , 18.65 ,  8.196, 12.05 , 13.49 ,\n",
       "       11.76 , 13.64 , 11.94 , 18.22 , 15.1  , 11.52 , 19.21 , 14.71 ,\n",
       "       13.05 ,  8.618, 10.17 ,  8.598,  9.173, 12.68 , 14.78 ,  9.465,\n",
       "       11.31 ,  9.029, 12.78 , 18.94 ,  8.888, 17.2  , 13.8  , 12.31 ,\n",
       "       16.07 , 13.53 , 18.05 , 20.18 , 12.86 , 11.45 , 13.34 , 25.22 ,\n",
       "       19.1  , 12.   , 18.46 , 14.48 , 19.02 , 12.36 , 14.64 , 14.62 ,\n",
       "       15.37 , 13.27 , 13.45 , 15.06 , 20.26 , 12.18 ,  9.787, 11.6  ,\n",
       "       14.42 , 13.61 ,  6.981,  9.876, 10.49 , 13.11 , 11.64 , 22.27 ,\n",
       "       11.34 ,  9.777, 12.63 , 14.26 , 10.51 ,  8.726, 11.93 ,  8.95 ,\n",
       "       14.87 , 17.95 , 11.41 , 18.66 , 24.25 , 14.5  , 13.37 , 13.85 ,\n",
       "       19.   , 19.79 , 12.19 , 15.46 , 16.16 , 15.71 , 18.45 , 12.77 ,\n",
       "       11.71 , 11.43 , 14.95 , 11.28 ,  9.738, 16.11 , 12.9  , 10.75 ,\n",
       "       11.9  , 11.8  , 14.44 , 13.74 ,  8.219,  9.731, 11.15 , 13.15 ,\n",
       "       12.25 , 17.68 , 16.84 , 12.06 , 10.9  , 11.75 , 19.19 , 19.59 ,\n",
       "       12.34 , 23.27 , 14.97 , 10.8  , 16.78 , 17.47 , 12.32 , 13.43 ,\n",
       "       11.08 , 10.66 ,  8.671,  9.904, 16.46 , 13.01 , 12.81 , 27.22 ,\n",
       "       21.09 , 15.7  , 15.28 , 10.08 , 18.31 , 11.81 , 12.3  , 14.22 ,\n",
       "        9.72 , 14.86 , 12.91 , 13.77 , 18.08 , 19.18 , 14.45 , 12.23 ,\n",
       "       17.54 , 23.29 , 13.81 , 12.47 , 15.12 , 17.01 , 15.27 , 20.58 ,\n",
       "       28.11 , 17.42 , 14.19 , 13.86 , 11.89 , 10.2  , 19.8  , 19.53 ,\n",
       "       13.65 , 13.56 , 10.18 , 15.75 , 14.34 , 10.44 , 15.   , 12.62 ,\n",
       "       12.83 , 17.05 , 11.32 , 11.22 , 20.51 ,  9.567, 14.03 , 23.21 ,\n",
       "       20.48 , 17.46 , 12.42 , 11.3  , 13.75 , 19.4  , 10.48 , 13.2  ,\n",
       "       12.89 , 10.65 , 20.94 , 11.5  , 19.73 , 17.3  , 19.45 , 13.96 ,\n",
       "       19.55 , 15.32 , 15.66 , 15.53 , 20.31 , 17.35 , 17.29 , 15.61 ,\n",
       "       17.19 , 20.73 , 10.6  , 13.59 , 12.87 , 10.71 , 14.29 , 11.29 ,\n",
       "       21.75 ,  9.742, 17.93 , 11.33 , 18.81 , 19.16 , 11.74 , 16.24 ,\n",
       "       12.58 , 11.26 , 11.37 , 14.41 , 14.96 , 12.95 , 11.85 , 12.72 ,\n",
       "       10.91 , 20.09 , 11.46 ,  9.   , 13.5  , 11.7  , 14.61 , 12.76 ,\n",
       "       11.54 ,  8.597, 12.49 ,  9.042, 12.43 , 10.25 , 20.16 , 20.34 ,\n",
       "       12.2  , 12.67 , 14.11 , 12.03 , 16.27 , 16.26 , 16.03 , 12.98 ,\n",
       "       11.25 , 17.06 , 12.99 , 18.77 , 10.05 , 23.51 ,  9.606, 11.06 ,\n",
       "       19.68 , 10.26 , 14.76 , 11.47 , 11.95 , 11.66 , 25.73 , 15.08 ,\n",
       "       11.14 , 12.56 , 13.87 ,  8.878,  9.436, 12.54 , 13.3  , 16.5  ,\n",
       "       13.4  , 20.44 , 20.2  , 12.21 , 21.71 , 22.01 , 16.35 , 15.19 ,\n",
       "       21.37 , 20.64 , 13.69 , 16.17 , 10.57 , 13.46 , 13.66 , 11.27 ,\n",
       "       11.04 , 12.39 , 14.6  , 13.88 ,  8.734, 15.49 , 21.61 , 12.1  ,\n",
       "       14.06 , 13.51 , 12.8  , 17.91 , 12.96 , 12.94 , 10.94 , 16.14 ,\n",
       "       12.85 , 12.27 , 11.36 ,  9.397, 15.13 ,  9.405, 15.5  , 12.7  ,\n",
       "       11.16 , 11.57 , 14.69 , 11.61 , 10.03 , 11.13 , 14.9  , 12.4  ,\n",
       "       18.82 , 13.98 , 14.04 , 14.02 , 10.97 , 17.27 , 13.78 , 18.03 ,\n",
       "       11.99 , 17.75 , 14.8  , 14.53 , 21.1  , 11.87 , 13.38 , 11.63 ,\n",
       "       13.21 ,  9.755, 17.08 , 27.42 , 14.4  , 13.24 , 13.14 ,  9.668,\n",
       "       17.6  , 11.62 ,  9.667, 12.04 , 14.92 , 10.88 , 14.2  , 13.9  ,\n",
       "       11.49 , 16.25 , 12.16 , 13.47 , 13.7  , 15.73 , 19.44 , 11.68 ,\n",
       "       16.69 , 17.85 , 18.01 , 13.16 , 12.65 , 18.49 , 20.59 , 15.04 ,\n",
       "       13.82 , 23.09 ,  9.268,  9.676, 12.22 , 16.3  , 14.81 , 15.05 ,\n",
       "       19.89 , 12.88 , 12.75 ,  9.295, 24.63 ,  9.847,  8.571, 13.94 ,\n",
       "       12.07 , 11.67 , 13.68 , 20.47 , 10.96 , 20.55 , 14.27 , 11.69 ,\n",
       "        7.729,  7.691, 14.47 , 14.74 , 13.62 , 10.32 ,  9.683, 10.82 ,\n",
       "       10.86 ,  9.333, 10.29 , 10.16 ,  9.423, 14.59 , 11.51 , 14.05 ,\n",
       "       11.2  , 15.22 , 20.92 , 21.56 , 20.13 , 16.6  , 20.6  ,  7.76 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radius_mean_unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see that in each of the cases, how many unique values are there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diagnosis_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(radius_mean_unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that in our discrete random variable of 'diagnosis', we have very limited values, only two in number and they are string in nature : ['B, 'M']\n",
    "\n",
    "Whereas, in our continous random variable of 'radius_mean', we can see the numbe rof unique values is huge and it's 456 which is not much smaller than 569, the number of rows in a column. Moreover, the nature or datatype of values is floating point. \n",
    "\n",
    "So, lets write this concept in the form of the code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_random_variable_type(column_name):\n",
    "    \n",
    "    if (type(data[column_name][0]) == str or type(data[column_name][0]) == int) and (len(data[column_name].unique()) < len(data[column_name])):\n",
    "        \n",
    "        return 'discrete'\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return 'continous'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function will take the name of the single column of the dataset and access the data of that column from the dataset and check the datatype or nature of the values inside that column and also the total number of unique values in that column and check the conditions for the random variable or that specific column being discrete or continous and returns the the type of random variable as strings 'discrete' or 'continous'. \n",
    "\n",
    "Now, the next task is to calculate the value of the best estimator of the population parameter of the probability distribution from which a specific column data is being sampled. In case of discrete random variables, it's pretty easy to determine that: If there are only two unique values in the column data, then it's binomial distribution and multinomial if there are more than two unique values. \n",
    "\n",
    "But, the real challenge lies in case of continous random variables, where it is difficult to determine that from which continous probability distibution the column data is being sampled. We have two options here: Either Normal (Symmetric) or Rayleigh (Skewed). So, in this case we have to find the best fit of our data on both the distributions and then select the distribution with the BESTEST FIT. \n",
    "\n",
    "Well, now the question arrises that how we are going to do that. We know that the Log Likelihood Function Value calculated on the data determines the quality of fit of data on the presumed distribution and if we maximize that value then we get the best fit of a specific probability distribution on the data. so, we first find the best fit of normal distibution on our continous data and then we find the best fit of Rayleigh Distribution on our continous data by maximizing the Log Likelihood Function Value for both the cases and because we have to select that distribution which gives us the bestest fit so we will end up selecting the distribution whose Log Likelihood Function value is maximum among both maxmized Log Likelihood Function Values. \n",
    "\n",
    "The problem is that in order to find the Log Likelihood Function value evaluated in case of fit of specific distribution on the data, we need the PDF of that distribution, which we know but we dont know the values of best estimators of population parameters:\n",
    "\\begin{equation}\n",
    "\\mu, \\sigma\n",
    "\\end{equation}\n",
    "of Normal Distribution and best estimator of population parameter, \n",
    "\\begin{equation}\n",
    "\\sigma (mode)\n",
    "\\end{equation}\n",
    "\n",
    "of Rayleigh Distribution. \n",
    "\n",
    "Now, how do we get the values of the best estimators of population parameters of respective distributions. Well, we know the answer to this question and that is by Maximum Log Likelihood Estimation. We know that the data in each column of this dataset is IID (Independent and Identically Distributed) therefore, for any random column of this dataset, X where X can be 'radius_mean' or 'testure_mean' or anything else, the Log Likelihood function is evaluated as: \n",
    "\n",
    "\\begin{equation}\n",
    "\\log_e L(\\mu,\\sigma) = \\log_e P(X=x_1\\cap X=x_2\\cap .........X=x_N)\n",
    "\\end{equation}\n",
    "For Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\log_e L(\\mu, \\sigma) = \\log_e\\prod\\limits_{i=0}^{N}\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\log_e L(\\mu, \\sigma) = \\sum\\limits_{i=0}^{N}\\log_e\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right)\n",
    "\\end{equation}\n",
    "Here, the population mean and population standard deviation estimator best estimate for normal distribution can be evaluated by taking the derivative of the above function with respect to $\\mu$ and $\\sigma$ and when we do that, that is:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial\\log_e L(\\mu, \\sigma)}{\\partial\\mu} = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial log_e L(\\mu, \\sigma)}{\\partial\\sigma} = 0\n",
    "\\end{equation}\n",
    "\n",
    "We get,\n",
    "\\begin{equation}\n",
    "\\mu_\\text{best,normal} = \\frac{\\sum\\limits_{i=0}^{N}{X_i}}{N}\n",
    "\\end{equation}\n",
    "\n",
    "And, \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_\\text{best,normal} = \\sqrt{\\frac{\\sum\\limits_{i=0}^{N}{(x_i-\\mu_\\text{best,normal})^2}}{N}}\n",
    "\\end{equation}\n",
    "\n",
    "Which is sample mean and sample standard deviation, therefore these are the best estimates of the population parameters of Normal Distribution for the best fit on our column data. \n",
    "\n",
    "Similarly, we can find the values of the best estimators of the population parameter of Rayliegh Distribution by following the above mentioned steps, and we get the following value for the best estimate in case of Rayleigh Distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_\\text{best,rayleigh} = \\sqrt{\\frac{\\sum\\limits_{i=0}^{N}{x_i^2}}{2N}}\n",
    "\\end{equation}\n",
    "\n",
    "So, these best estimates of the MVU Estimators of respective population parameters of respective Probability Distributions will maximize the Log Likelihood Function values for respective probability distributions and hence will result in best fit of respective distributions. Now, to determine the bestest fit among both of the distributions, we have to select that distribution for which the Log Likelihood Function is the highest among both already maximized values. \n",
    "\n",
    "Now, in our case N = 569, so the best estimates of the MVU estimators of the population parameters of respective probability distributions are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu_\\text{best,normal} = \\frac{\\sum\\limits_{i=0}^{569}{x_i}}{569}\n",
    "\\end{equation}\n",
    "\n",
    "And, \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_\\text{best,normal} = \\sqrt{\\frac{\\sum\\limits_{i=0}^{569}{(x_i-\\mu_\\text{best,normal})^2}}{569}}\n",
    "\\end{equation}\n",
    "\n",
    "For Normal Distribution\n",
    "\n",
    "And, \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_\\text{best,rayleigh} = \\sqrt{\\frac{\\sum\\limits_{i=0}^{569}{x_i^2}}{2*569}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Therefore, the maximized values of Log Likelihood Function for the case of both the probability distributions is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "L_\\text{max,normal} = \\log_e L(\\mu_\\text{best,normal},\\sigma_\\text{best,normal})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L_\\text{max,normal} = \\sum\\limits_{i=0}^{569}\\log_e\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_\\text{best,normal}}e^\\frac{(x_i-\\mu_\\text{best,normal})^2}{2\\sigma_\\text{best,normal}^2}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "For Normal Distribution\n",
    "\n",
    "And,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, maximized value of Log Likelihood Function for Rayleigh Distribution can be calculated as we have just calculated for Normal Distribution. \n",
    "\\begin{equation}\n",
    "L_\\text{max,rayleigh} = \\log_e L(\\sigma_\\text{best,rayleigh})\n",
    "\\end{equation}\n",
    "\n",
    "Now, the column data will be considered to be sampled from Normal Distribution if:\n",
    "\n",
    "\\begin{equation}\n",
    "L_\\text{max,normal} > L_\\text{max,rayleigh}\n",
    "\\end{equation}\n",
    "\n",
    "else:\n",
    "\n",
    "column data will be considered to be sampled from Rayleigh Distribution.\n",
    "\n",
    "Therefore, in order to do all that, we are going to create following set of functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_L_max_normal(column_name):\n",
    "    \n",
    "    mu_best_normal = data[column_name].mean()\n",
    "    \n",
    "    sigma_best_normal = data[column_name].std()\n",
    "    \n",
    "    \n",
    "    #write the expression for evaluating the maximized value of Log Likelihood Function for Normal Distribution\n",
    "    L_max_normal = np.log((math.e ** -(((data[column_name] - mu_best_normal) ** 2) / (2 * sigma_best_normal ** 2))) / ((2 * math.pi) ** 0.5 * sigma_best_normal))\n",
    "    print(L_max_normal)\n",
    "    return L_max_normal.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_L_max_normal(column_name):\n",
    "    \n",
    "    mu_best_normal = data[column_name].mean()\n",
    "    \n",
    "    sigma_best_normal = data[column_name].std()\n",
    "    \n",
    "    \n",
    "    #write the expression for evaluating the maximized value of Log Likelihood Function for Normal Distribution\n",
    "    L_max_normal = np.log((math.e ** -(((data[column_name] - mu_best_normal) ** 2) / (2 * sigma_best_normal ** 2))) / ((2 * math.pi) ** 0.5 * sigma_best_normal))\n",
    "    print(L_max_normal)\n",
    "    return L_max_normal.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_L_max_rayleigh(column_name):\n",
    "    \n",
    "    \n",
    "    #calculate the value of best estimate of MVU Estimator of sigma of Rayleigh Distribution here\n",
    "    sigma_best_rayleigh = (((data[column_name] ** 2).sum()) / (data.shape[0] * 2)) ** 0.5\n",
    "    \n",
    "    \n",
    "    #write the expression for evaluating the maximized value of Log Likelihood Function for Rayleigh Distribution\n",
    "    L_max_rayleigh = np.log((data[column_name] / (sigma_best_rayleigh ** 2)) * (math.e ** -((data[column_name] ** 2) / (2 * sigma_best_rayleigh ** 2))))\n",
    "    \n",
    "    return L_max_rayleigh.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_distribution_type(column_name):\n",
    "    \n",
    "    #Write the code here to first determine type of random variable : Either Discrete or Continous\n",
    "    column_type = determine_random_variable_type(column_name)\n",
    "    \n",
    "    if column_type == 'discrete':\n",
    "        \n",
    "        #if the above condition is true, then write the code to determine whether it's binomial or multinomial\n",
    "        #and return the strings 'binomial' or 'multinomial' accordingly\n",
    "        if len(data[column_name].unique()) == 2:\n",
    "            return 'binomial'\n",
    "        else:\n",
    "            return 'multinomial'\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #write the code to find L_max_normal here \n",
    "        L_max_normal = calculate_L_max_normal(column_name)\n",
    "        \n",
    "        #write the code to find the L_max_rayleigh here \n",
    "        L_max_rayleigh = calculate_L_max_rayleigh(column_name)\n",
    "        \n",
    "        \n",
    "        if L_max_normal > L_max_rayleigh:\n",
    "            \n",
    "            return 'normal'\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return 'rayleigh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnosis               : binomial\n",
      "radius_mean             : normal\n",
      "texture_mean            : normal\n",
      "perimeter_mean          : normal\n",
      "area_mean               : rayleigh\n",
      "smoothness_mean         : normal\n",
      "compactness_mean        : rayleigh\n",
      "concavity_mean          : normal\n",
      "concave points_mean     : normal\n",
      "symmetry_mean           : normal\n",
      "fractal_dimension_mean  : normal\n",
      "radius_se               : rayleigh\n",
      "texture_se              : rayleigh\n",
      "perimeter_se            : rayleigh\n",
      "area_se                 : rayleigh\n",
      "smoothness_se           : rayleigh\n",
      "compactness_se          : rayleigh\n",
      "concavity_se            : normal\n",
      "concave points_se       : normal\n",
      "symmetry_se             : normal\n",
      "fractal_dimension_se    : rayleigh\n",
      "radius_worst            : normal\n",
      "texture_worst           : normal\n",
      "perimeter_worst         : normal\n",
      "area_worst              : rayleigh\n",
      "smoothness_worst        : normal\n",
      "compactness_worst       : rayleigh\n",
      "concavity_worst         : normal\n",
      "concave points_worst    : normal\n",
      "symmetry_worst          : normal\n",
      "fractal_dimension_worst : normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manav\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\series.py:679: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for i in data.columns:\n",
    "    print(f'{i}'.ljust(23), f': {determine_distribution_type(i)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
